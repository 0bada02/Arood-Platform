{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arood Version3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwlvjSR-DS15"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "23FSFg5t6fc1",
    "outputId": "9ebe7f56-efc0-4766-c022-9d4cf6fe5114"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from random import shuffle\n",
    "from pyarabic import araby\n",
    "from tensorflow.keras.layers import GRU, Embedding, Dense, Input, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_physical_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in the file final_baits it is splitted into two files for training train.txt and testing test.txt and the labels of the meters are saved in the file labels.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read label names from a file and clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_baits/labels.txt', 'r') as f:\n",
    "  label2name = f.readlines()\n",
    "  label2name = [name.replace('\\n', '') for name in label2name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_prosody(text: str) -> str:\n",
    "    # Rule 1: Replace tanween with silent noon\n",
    "    tanween_to_noon = {\n",
    "        r'[ً]': 'ن',  # Tanween Fatha\n",
    "        r'[ٌ]': 'ن',  # Tanween Damma\n",
    "        r'[ٍ]': 'ن',  # Tanween Kasra\n",
    "    }\n",
    "    for pattern, replacement in tanween_to_noon.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "\n",
    "    # Rule 2: Handle shadda by duplicating letters\n",
    "    text = re.sub(r'(.)ّ', r'\\1\\1', text)  # Replace shadda with duplicated letters\n",
    "\n",
    "    # Rule 3: Special handling: Saturate doubled letters at the end of the first hemistich\n",
    "    text = re.sub(r'(.)ّ$', r'\\1\\1', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Rule 4: Add Alif in specific contexts\n",
    "    alif_replacements = {\n",
    "        r'هذا': 'هاذا',\n",
    "        r'هذه': 'هاذه',\n",
    "        r'هذان': 'هاذان',\n",
    "        r'هذين': 'هاذين',\n",
    "        r'ذلك': 'ذالك',\n",
    "        r'الله': 'اللاه',\n",
    "        r'الرحمن': 'اَرْرحمان',\n",
    "        r'إله': 'إلاه',\n",
    "        r'لكنْ': 'لاكنْ',\n",
    "        r'لكنَّ': 'لاكنْنَ',\n",
    "        r'طه': 'طاها'\n",
    "    }\n",
    "    for original, prosodic in alif_replacements.items():\n",
    "        text = text.replace(original, prosodic)\n",
    "\n",
    "    # Rule 5: Handle Solar and Lunar Lam\n",
    "    # Define solar and lunar letters\n",
    "    solar_letters = r'تثدذرشصضطظلن'\n",
    "    lunar_letters = r'ابجحخعغفقكملوه'\n",
    "\n",
    "    # Remove the \"ل\" in \"ال\" when followed by solar letters\n",
    "    text = re.sub(r'\\bال([' + re.escape(solar_letters) + r'])', r'ا\\1', text)\n",
    "\n",
    "    # Ensure \"ال\" remains unchanged for lunar letters\n",
    "    text = re.sub(r'\\bال([' + re.escape(lunar_letters) + r'])', r'ال\\1', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKhqB_MfCjEP"
   },
   "source": [
    "Extracts labeled text data from the provided file.\n",
    "Preprocesses text by removing diacritics and unwanted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T7MjMLLn6gtK"
   },
   "outputs": [],
   "source": [
    "def extract_data(path, on_shatrs=False):\n",
    "    global vocab\n",
    "    text = \"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    # Read the file with UTF-8 encoding\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        t = file.read()\n",
    "\n",
    "    t = preprocess_prosody(t)\n",
    "    t = araby.strip_tatweel(t)\n",
    "    \n",
    "    # Remove unwanted characters\n",
    "    excluded_chars = '!()*-ـ.:=o[]«»;؛,،~?؟\\u200f\\ufeffـ'\n",
    "    cleaned_text = ''.join([char for char in t if char not in excluded_chars])\n",
    "    \n",
    "    text += cleaned_text\n",
    "    baits = cleaned_text.split('\\n')\n",
    "    for line in baits:\n",
    "        if len(line) <= 1:  # Skip empty or short lines\n",
    "            continue\n",
    "        label, bait = line.split(' ', 1)  # Split label and text\n",
    "        label = int(label)\n",
    "        bait = bait.strip()\n",
    "        if on_shatrs:\n",
    "            # Further split text into parts (shatrs)\n",
    "            shatrs = bait.split('#')\n",
    "            for shatr in shatrs:\n",
    "                X.append(shatr.strip())\n",
    "                y.append(label)\n",
    "        else:\n",
    "            X.append(bait.strip())\n",
    "            y.append(label)\n",
    "    \n",
    "    # Create a sorted vocabulary from the dataset\n",
    "    vocab = sorted(set(' '.join(X)))\n",
    "\n",
    "    # Shuffle the data to avoid order bias\n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Train Data & Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File paths for training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join('./final_baits', 'train.txt')\n",
    "test_file = os.path.join('./final_baits', 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and preprocess the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extract_data(train_file, on_shatrs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first few data samples for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "كللما حننتْ لأرضِ المُنحنى # وكَلاها أقرحَ السسَوْقُ كُلاها   ramal\n",
      "فكاتب يقام إجلالا له # وكاتب لا نستحى أن نصفعه   rajaz\n",
      "فَيا لِلنَصارى إِذا أَمسَكوا # وَيا لِليَهودِ إِذا أَسبَتوا   mutakareb\n",
      "كأن ذاك اشرار من ذهب # قُراضةن تستطير من نُقَر   munsareh\n",
      "أنا مفتاحُ المَلاهِي والطربْ # هيئتي ظَرْفن وأحوالي عَجَب   ramal\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(X[i], ' ', label2name[y[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ROef8aerf8ar"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and preprocess the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = extract_data(test_file, on_shatrs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each character in the vocabulary to an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i+1 for i, u in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts text data to sequences of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "63NiojywQ18F"
   },
   "outputs": [],
   "source": [
    "def to_sequences(X):\n",
    "  X = [[char2idx[char] for char in line] for line in X]\n",
    "  X = pad_sequences(X, padding='post', value=0, maxlen = 100)\n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text data into sequences of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = to_sequences(X_train)\n",
    "X_valid = to_sequences(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert labels to numpy arrays for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uHdRK4cCrGJ"
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "p3u3OxEcBfJ2"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input((100,)),  # Input layer with maxlen=100\n",
    "    Embedding(len(char2idx) + 1, 256),  # Embedding layer with vocab size + 1\n",
    "    Bidirectional(GRU(units=256, return_sequences=True)),  # First Bi-GRU layer\n",
    "    Bidirectional(GRU(units=256, return_sequences=True)),  # Second Bi-GRU layer\n",
    "    Bidirectional(GRU(units=256)),  # Third Bi-GRU layer\n",
    "    Dense(128, activation='relu'),  # Dense layer for feature extraction\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Dense(len(label2name), activation='softmax')  # Output layer for classification\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with Adam optimizer and categorical crossentropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "7imJBjJHxK1-",
    "outputId": "42a9b432-3cec-44d5-9af7-3da1eb72105f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 256)          11264     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 512)         789504    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 100, 512)         1182720   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 512)              1182720   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 14)                1806      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,233,678\n",
      "Trainable params: 3,233,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify model input-output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "nPyUsGe_u9tw",
    "outputId": "921fdf7c-c163-4981-b02b-2402be47d539"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 14])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.zeros((10, 100))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define callbacks for learning rate adjustment and model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nEHRYgLhkozM"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.0001, min_lr=0.0001),\n",
    "    tf.keras.callbacks.ModelCheckpoint('full_verse.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "52Ew-5ZyC3Nf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.61423, saving model to full_verse.keras\n",
      "313/313 [==============================] - 36s 97ms/step - loss: 2.1109 - accuracy: 0.2736 - val_loss: 1.1936 - val_accuracy: 0.6142 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.61423 to 0.87551, saving model to full_verse.keras\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.7618 - accuracy: 0.7720 - val_loss: 0.4289 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.87551 to 0.91498, saving model to full_verse.keras\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.3800 - accuracy: 0.8934 - val_loss: 0.3124 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.91498 to 0.93026, saving model to full_verse.keras\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.2677 - accuracy: 0.9292 - val_loss: 0.2466 - val_accuracy: 0.9303 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.93026 to 0.93761, saving model to full_verse.keras\n",
      "313/313 [==============================] - 31s 101ms/step - loss: 0.2200 - accuracy: 0.9412 - val_loss: 0.2342 - val_accuracy: 0.9376 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.93761 to 0.94115, saving model to full_verse.keras\n",
      "313/313 [==============================] - 31s 101ms/step - loss: 0.1831 - accuracy: 0.9516 - val_loss: 0.2290 - val_accuracy: 0.9412 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.94115 to 0.94186, saving model to full_verse.keras\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.1531 - accuracy: 0.9601 - val_loss: 0.2317 - val_accuracy: 0.9419 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.94186\n",
      "313/313 [==============================] - 31s 97ms/step - loss: 0.1367 - accuracy: 0.9628 - val_loss: 0.2241 - val_accuracy: 0.9399 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.94186 to 0.94242, saving model to full_verse.keras\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.1182 - accuracy: 0.9680 - val_loss: 0.2303 - val_accuracy: 0.9424 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.94242\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.1033 - accuracy: 0.9713 - val_loss: 0.2258 - val_accuracy: 0.9416 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.94242 to 0.95162, saving model to full_verse.keras\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.0598 - accuracy: 0.9841 - val_loss: 0.2079 - val_accuracy: 0.9516 - lr: 1.0000e-04\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.95162 to 0.95233, saving model to full_verse.keras\n",
      "313/313 [==============================] - 30s 96ms/step - loss: 0.0379 - accuracy: 0.9903 - val_loss: 0.2155 - val_accuracy: 0.9523 - lr: 1.0000e-04\n",
      "Epoch 13/15\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.95233\n",
      "313/313 [==============================] - 30s 96ms/step - loss: 0.0290 - accuracy: 0.9927 - val_loss: 0.2260 - val_accuracy: 0.9520 - lr: 1.0000e-04\n",
      "Epoch 14/15\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.95233\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.0229 - accuracy: 0.9940 - val_loss: 0.2391 - val_accuracy: 0.9519 - lr: 1.0000e-04\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.95233 to 0.95303, saving model to full_verse.keras\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.0184 - accuracy: 0.9956 - val_loss: 0.2480 - val_accuracy: 0.9530 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e415f7850>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data= (X_valid, y_valid), epochs = 15, batch_size= 128, shuffle = True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/260 [==============================] - 7s 24ms/step\n",
      "Test Accuracy: 0.9518\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       731\n",
      "           1       0.94      0.94      0.94       760\n",
      "           2       0.97      0.98      0.98       758\n",
      "           3       0.93      0.93      0.93       178\n",
      "           4       0.98      0.94      0.96       681\n",
      "           5       0.95      0.83      0.89       230\n",
      "           6       0.95      0.96      0.95       303\n",
      "           7       0.97      0.96      0.96       719\n",
      "           8       0.97      0.98      0.98       752\n",
      "           9       0.97      0.98      0.97       759\n",
      "          10       0.98      0.99      0.99       752\n",
      "          11       0.96      0.97      0.97       769\n",
      "          12       0.91      0.95      0.93       168\n",
      "          13       0.90      0.88      0.89       756\n",
      "\n",
      "    accuracy                           0.95      8316\n",
      "   macro avg       0.95      0.95      0.95      8316\n",
      "weighted avg       0.95      0.95      0.95      8316\n",
      "\n",
      "Confusion Matrix:\n",
      "[[679   7  11   3   4   0   0   0   2   0   1   7   1  16]\n",
      " [  4 713   2   0   0   0   0   1   1   2   2  10   0  25]\n",
      " [  5   0 745   0   0   1   0   2   1   2   1   1   0   0]\n",
      " [  2   0   0 166   0   1   0   1   0   3   0   1   2   2]\n",
      " [ 11   0   1   2 643   2   8   0   3   3   0   1   1   6]\n",
      " [  8   0   1   4   1 192   0   8   0   5   3   0   5   3]\n",
      " [  0   0   1   0   0   1 291   0   0   1   0   1   4   4]\n",
      " [  8   1   0   2   1   4   3 688   0   2   4   1   2   3]\n",
      " [  0   6   0   0   3   0   2   1 736   1   0   0   0   3]\n",
      " [  0   1   0   0   5   2   1   3   0 741   1   2   0   3]\n",
      " [  0   1   0   0   0   0   0   0   1   0 747   1   0   2]\n",
      " [  1   3   2   0   0   0   1   2   6   1   0 749   0   4]\n",
      " [  2   0   2   2   0   0   0   1   0   0   0   0 160   1]\n",
      " [ 46  26   3   0   1   0   1   1   7   1   0   4   1 665]]\n"
     ]
    }
   ],
   "source": [
    "X_test = to_sequences(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display classification metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify a single sentence using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-Q-Texz3DQsH"
   },
   "outputs": [],
   "source": [
    "def classify(sentence):\n",
    "    sentence = preprocess_prosody(sentence)\n",
    "    sentence = araby.strip_tatweel(sentence)\n",
    "    sequence = [char2idx[char] for char in sentence]\n",
    "    sequence = pad_sequences([sequence], maxlen = X_train.shape[1], padding='post', value=0)\n",
    "\n",
    "    pred = model.predict(sequence)[0]\n",
    "    print(label2name[np.argmax(pred, 0).astype('int')], np.max(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwZQrxhdDV4r"
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "GMgcfGkZRLF2",
    "outputId": "a0e20dc1-cf07-421e-b585-7948168dd485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n",
      "ramal 0.9303919\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "kamel 0.9826461\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "kamel 0.99910456\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "taweel 0.8783224\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "mujtath 0.9995054\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "khafeef 0.9937691\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "khafeef 0.9998223\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "mutakareb 0.99994004\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "mutakareb 0.99996233\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "khafeef 0.99200964\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "mujtath 0.99999654\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "taweel 0.9996996\n"
     ]
    }
   ],
   "source": [
    "classify(\"ما تردون على هذا المحب # دائبا يشكو إليكم في الكتب\")\n",
    "classify(\"ولد الهدى فالكائنات ضياء # وفم الزمان تبسم وسناء\")\n",
    "classify(\"لك يا منازل في القلوب منازل # أقفرت أنت وهن منك أواهل\")\n",
    "classify(\"ومن لم يمت بالسيف مات بغيره # تعددت الأسباب والموت واحد\")\n",
    "classify(\"أنا النبي لا كذب # أنا ابن عبد المطلب\")\n",
    "classify(\"قَد تَقَطَرنَ بالعبيرِ ومَسكٍ # وَتَكَبَينَ بالكباءِ ذكيا\")\n",
    "classify(\"رُبَّما ضَربَةٍ بسيفٍ صَقِيلٍ # دُونَ بُصرَى وَطَعْنَةٍ نَجلاءِ\")\n",
    "classify(\"أَيا هِندُ لا تَنكِحي بَوهَةَ # عَلَيهِ عَقيقَتُهُ أَحسَبا\")\n",
    "classify(\"أَكَلتُ شَبابي فَأَفنَيتُهُ # وَأَفنَيتُ بَعدَ شُهورٍ شُهورا\")\n",
    "classify(\"بان شبابٌ لمَّا يكن شابا # ويحي ولم أقضِ منه آرابا\")\n",
    "classify(\"عوجوا إِلى بَيتِ عَمرو # إِلى سَماعٍ وَخَمرِ\")\n",
    "classify(\"إِذا ما اِتَّقَينا رَمقَةً مِن مُبَلِّغٍ # فَأَعيُنُنا عَنّا تُجيبُ وَتَفهَمُ\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AraMeter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
